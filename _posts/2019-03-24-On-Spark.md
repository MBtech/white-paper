---
layout: post
title: "On Spark"
categories:
  - guide
tags:
  - guide
---
Updated on: 09-04-2019
These notes will be consistently updated as I learn new things while experimenting with spark

## Partitions and executors
One partition is executed by one executor so your number of partitions should be equal to the number of executors with which you are running the job.

## GraphX and Java
Spark supposedly has an Alpha API in Java to use for GraphX. However, If you want to change the code for the GraphX data structures and builtin algorithms you would have to
do the changes in Scala.
There is a GraphFrames project to work with Graphs on Spark with Java as well. However, it has some limitation. Please take a look at the latest update on it to see those limitations.

## Figuring out Location of a Partition
So, I believe the only way to know what executor a partition is executed at is to use org.apache.spark.SparkEnv to access the BlockManager that corresponds to a single executor. That's exactly how Spark knows/tracks executors (by their BlockManagers).

You could write a org.apache.spark.scheduler.SparkListener that would intercept onExecutorAdded, onBlockManagerAdded and their \*Removed counterparts to know how to map executors to BlockManagers (but believe SparkEnv is enough).

## Building a Runnable Distribution
You can build spark runnable using:

`./dev/make-distribution.sh --name custom-spark --pip --r --tgz -Psparkr -Phadoop-2.7 -Phive -Phive-thriftserver -Pmesos -Pyarn -Pkubernetes`

If you want to build a standalone version without the support for sparkr simply use the following:

`./dev/make-distribution.sh --name custom-spark --pip --tgz -Phadoop-2.7`

The make-distribution.sh script always does a `mvn clean package`. This can take a while, so you can edit it to instead for a `mvn package`.

The script produces a `spark-<spark-version>-SNAPSHOT-custom-spark.tgz` package that you can untar to get a distribution that similar to what you would down from Spark Download page.

Take a look at this documentation for more details [guide](https://spark.apache.org/docs/latest/building-spark.html).

**References:**

1. https://stackoverflow.com/questions/30725687/how-to-know-which-worker-a-partition-is-executed-at
